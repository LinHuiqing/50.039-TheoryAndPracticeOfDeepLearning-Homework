{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW11 - Descriptive Notebook\n",
    "\n",
    "Note: Updated, to remove flattening in trainer function.\n",
    "\n",
    "In this homework notebook, we will create a Wasserstein GAN with Convolution and Transpose Convolution layers, to be used on the MNIST dataset.\n",
    "\n",
    "Get familiar with the code and write a small report (2 pages max), with answers to the questions listed at the end of the notebook.\n",
    "\n",
    "**The report must be submitted in PDF format, before April 18th, 11.59pm!**\n",
    "\n",
    "Do not forget to write your name and student ID on the report.\n",
    "\n",
    "You may also submit your own copy of the notebook along with the report. If you do so, please add your name and ID to the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name:\n",
    "# Student ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transform to be applied to dataset\n",
    "# - Tensor conversion\n",
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST train dataset\n",
    "mnist = torchvision.datasets.MNIST(root = './data/',\n",
    "                                   train = True,\n",
    "                                   transform = transform,\n",
    "                                   download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "batch_size = 32\n",
    "data_loader = torch.utils.data.DataLoader(dataset = mnist,\n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic model as a set of Conv2d layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task1:** Rewrite the Critic model below, so that it uses Conv2d layers instead of fully connected ones shown in class.\n",
    "\n",
    "You may look for inspiration in the encoder models used in Notebook 3 (W11S1 lecture).\n",
    "\n",
    "The critic should use three Conv2d layers with progressive downsampling.\n",
    "\n",
    "We do not advise to add more layers to the mix (BatchNorm, Dropout, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic\n",
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_size):\n",
    "        \"\"\"\n",
    "        Only forced parameter will be the image size, set to 28.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator model as a set of Transposed Conv2d layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task2:** Rewrite the Generator model below, so that it uses Transposed Conv2d layers instead of fully connected ones shown in class.\n",
    "\n",
    "You may look for inspiration in the encoder models used in Notebooks 2 and 3 (W11S1 lecture).\n",
    "\n",
    "The critic should use three Transposed Conv2d layers with progressive upsampling.\n",
    "\n",
    "We do not advise to add more layers to the mix (BatchNorm, Dropout, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_size, image_size):\n",
    "        \"\"\"\n",
    "        Only forced parameters will be the image size, set to 28,\n",
    "        and the latent size set to 64.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer function\n",
    "\n",
    "**Task 3:** Decide on a number of iterations num_epochs for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for model generation and training\n",
    "latent_size = 64\n",
    "image_size = 28\n",
    "num_epochs = None\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create discriminator model\n",
    "f = Critic(image_size)\n",
    "f.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator model\n",
    "G = Generator(latent_size, image_size)\n",
    "G.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses and optimizers\n",
    "d_optimizer = torch.optim.Adam(f.parameters(), lr = 0.0002)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr = 0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# History trackers for training curves\n",
    "# Keeping track of losses\n",
    "d_losses = np.zeros(num_epochs)\n",
    "g_losses = np.zeros(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: running the cell below (our trainer function) will take a long time!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(data_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        \n",
    "        # 1. Send image to device\n",
    "        images = Variable(images, requires_grad = True)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        PART 1: TRAIN THE CRITIC\n",
    "        \"\"\"\n",
    "\n",
    "        # 2. Compute mean of critic decisions using real images\n",
    "        outputs_real = f(images)\n",
    "        \n",
    "        # 2.bis. Compute mean of critic decisions using fake images\n",
    "        z = torch.randn(batch_size, latent_size).to(device)\n",
    "        z = Variable(z)\n",
    "        fake_images = G(z)\n",
    "        outputs_fake = f(fake_images)\n",
    "        \n",
    "        # 3. Compute gradient regularization\n",
    "        real_grad_out = Variable(Tensor(images.size(0), 1).fill_(1.0), requires_grad = False).to(device)\n",
    "        real_grad = autograd.grad(outputs_real, images, real_grad_out, create_graph = True, \\\n",
    "                                  retain_graph = True, only_inputs = True)[0]\n",
    "        real_grad_norm = real_grad.view(real_grad.size(0), -1).pow(2).sum(1)**3\n",
    "        fake_grad_out = Variable(Tensor(fake_images.size(0), 1).fill_(1.0), requires_grad = False).to(device)\n",
    "        fake_grad = autograd.grad(outputs_fake, fake_images, fake_grad_out, create_graph = True, \\\n",
    "                                  retain_graph = True, only_inputs = True)[0]\n",
    "        fake_grad_norm = fake_grad.view(fake_grad.size(0), -1).pow(2).sum(1)**3\n",
    "        reg_term = torch.mean(real_grad_norm + fake_grad_norm)\n",
    "        \n",
    "        # 4. Backprop and optimize for f\n",
    "        # Loss is simply the difference between means, plus regularization term\n",
    "        # Remember to reset gradients for both optimizers!\n",
    "        d_loss = -torch.mean(outputs_real) + torch.mean(outputs_fake) + reg_term\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # 4.bis. Optional, weight clipping on critic\n",
    "        # (Mentioned in WGAN paper)\n",
    "        for p in f.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        PART 2: TRAIN THE GENERATOR\n",
    "        \"\"\"\n",
    "\n",
    "        # 5. Generate fresh noise samples and produce fake images\n",
    "        z = torch.randn(batch_size, latent_size).cuda()\n",
    "        z = Variable(z)\n",
    "        fake_images = G(z)\n",
    "        outputs = f(fake_images)\n",
    "        \n",
    "        # 6. Loss for G\n",
    "        g_loss = - torch.mean(outputs)\n",
    "        \n",
    "        # 7. Backprop and optimize G\n",
    "        # Remember to reset gradients for both optimizers!\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        PART 3: UPDATE STATISTICS FOR VISUALIZATION LATER\n",
    "        \"\"\"\n",
    "        \n",
    "        # 8. Update the losses and scores for mini-batches\n",
    "        d_losses[epoch] = d_losses[epoch]*(i/(i+1.)) \\\n",
    "            + d_loss.item()*(1./(i+1.))\n",
    "        g_losses[epoch] = g_losses[epoch]*(i/(i+1.)) \\\n",
    "            + g_loss.item()*(1./(i+1.))\n",
    "        \n",
    "        # 9. Display\n",
    "        if (i+1) % 200 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}' \n",
    "                  .format(epoch, num_epochs, i+1, total_step, d_loss.item(), g_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display losses for both the generator and discriminator\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs + 1), d_losses, label = 'd loss')\n",
    "plt.plot(range(1, num_epochs + 1), g_losses, label = 'g loss')    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a few fake samples (5 of them) for visualization\n",
    "n_samples = 5\n",
    "z = torch.randn(n_samples, latent_size).cuda()\n",
    "z = Variable(z)\n",
    "fake_images = G(z)\n",
    "fake_images = fake_images.cpu().detach().numpy().reshape(n_samples, 28, 28)\n",
    "print(fake_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display\n",
    "plt.figure()\n",
    "plt.imshow(fake_images[0])\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.imshow(fake_images[1])\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.imshow(fake_images[2])\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.imshow(fake_images[3])\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.imshow(fake_images[4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions and expected answers for the report\n",
    "\n",
    "1. Copy and paste the code for your Critic class. Briefly explain your choice of architecture.\n",
    "\n",
    "2. Copy and paste the code for your Generator class. Briefly explain your choice of architecture.\n",
    "\n",
    "3. For how many iterations did you have to train when using Wasserstein with Conv/TransposeConv layers to get plausible images from the generator? Is it training faster than the Fully Connected Wasserstein/Vanilla GAN?\n",
    "\n",
    "4. Display some samples generated by your trained generator. Do they look plausible?\n",
    "\n",
    "5. Let us assume we use Conv2d layers in the Critic. We do NOT use Transposed Conv2d layers, but only Fully Connected layers in the Generator. Would the GAN still be able to train both models or would it encounter difficulties? Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}